<!DOCTYPE html>
<html>
<head>
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-501N4WR52H"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-501N4WR52H');
    </script>

    <meta charset="UTF-8">
    <title>Jeffrey S. Seely</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="...">

    <link rel="stylesheet" href="/css/style.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŽˆ</text></svg>">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:wght@300;400&family=Source+Sans+Pro:wght@300;400&family=Source+Serif+4:wght@300;400&family=Source+Serif+Pro:wght@300;400&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI" crossorigin="anonymous">

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t" crossorigin="anonymous"></script>

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
</head>
<body>

    <div class="container">
        <h1>Spelling Out the Equivalence of RNNs, CNNs, and Autoregressive Models in the Linear Case</h1>
        <p>This note spells out the equivalence of convolution, state-space, and autoregressive architectures for linear dynamical systems. This is a well-known exercise in engineering but less-known in machine learning. This exercise may be useful for improving intuition of sequence modeling in general, and for making sense of the RNN vs CNN vs AR architecture debates in ML.</p>
<p><img src="/images/archs.png" alt="Architectures"></p>
<p>We have a sequence of inputs $u(t)\in\mathbb{R}^m$ and outputs $y(t)\in\mathbb{R}^p$ indexed by time $t\in\mathbb{Z}$.</p>
<p>There are three main architectures for modeling of the current $y(t)$.</p>
<ul>
<li><strong>Convolutional</strong>: $y(t)$ depends on current and past inputs $u(t), u(t-1), u(t-2), \dots$</li>
<li><strong>Autoregressive</strong>: $y(t)$ depends on past outputs $y(t-1), y(t-2), \dots, y(t-T)$</li>
<li><strong>State-space</strong>: $y(t)$ depends on state variable $x(t)$ which in turn depends only on current input $u(t)$ and previous state $x(t-1)$.</li>
</ul>
<p>When these arechitectures are linear they become effectively interchangeable, despite appearances.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="the-six-lti-representations">The six LTI Representations</h2>
<p><img src="/images/ltireps.png" alt="linear systems representations"></p>
<!-- raw HTML omitted -->
<p>To be consistent with the engineering literature on LTI systems, and for completeness, we&rsquo;ll go through <em>six</em> representations of a discrete-time linear time-invariant system (figure from [1]).</p>
<p><strong>Architecture representations</strong>:</p>
<ol>
<li>State-space (aka linear RNN)</li>
<li>Convolution (aka impulse-response)</li>
<li>Autoregressive (aka difference equations)</li>
</ol>
<p><strong>Transfer Function representations</strong>:</p>
<ol start="4">
<li>Transfer function $H(q)$ of shift operator $q$</li>
<li>$z$-Transform transfer function $H(z)$ of complex variable $z$</li>
<li>DTFT transfer function $H(\omega)$ of complex variable $\omega$</li>
</ol>
<p>The transfer function representations are essentially different viewpoints of the autoregressive representation. They&rsquo;re a bit more abstract, but are still heavily emphasized in engineering texts.</p>
<p>Let&rsquo;s go through defining each representation, and along the way build the machinery for converting between them.</p>
<h2 id="1-state-space">1. State-space</h2>
<p>It&rsquo;s easiest to start with the state-space (SS) representation.</p>
<p>The SS representation is parametrized by matrices $A,B,C,D$. A.k.a. <strong>a linear RNN</strong>,</p>
<p>\begin{align}
x(t+1) = Ax(t) + Bu(t) \\
y(t) = Cx(t) + Du(t)
\end{align}</p>
<h2 id="2-convolution">2. Convolution</h2>
<p>The convolution (or impulse-response) representation is given by
\begin{align}
y(t) &amp; = h(t) \ast u(t) \
&amp; = \sum_{j=t}^{\infty} h(i) u(t-i)
\end{align}</p>
<p>Where $\ast$ represents a discrete convolution and $h(t)$ is a $p\times m$ matrix that varies with $t$.</p>
<h4 id="from-ss-rightarrow-conv">From SS $\rightarrow$ Conv</h4>
<p>Given a state-space representation, we convert to a convolution representation by assuming a zero initial state $x(0) = 0$. This amounts to solving the recurrance equation (unrolling the SS model) and obtaining,
\begin{align}
y(t) = \sum_{i=1}^\infty CA^{i-1} B u(t-i) + Du(t),
\end{align}
which implies convolution parameters,
\begin{align}
h(t) = \begin{cases}
D &amp; t=0 \\
CA^{t-1}B &amp; t \geq 1.
\end{cases}
\end{align}</p>
<p>The convolution has infinite receptive field <em>but finite parameters</em> &ndash; the &ldquo;weights&rdquo; are parametrized by $t$. In the case that $A$ is <a href="https://en.wikipedia.org/wiki/Nilpotent_matrix">nilpotent</a> then $h(t)$ does go to zero for large enough $t$ thus giving a finite receptive field but this is not common.</p>
<h2 id="3-autoregressive">3. Autoregressive</h2>
<p>We set $p=m=1$ in the following for notational compactness. The autoregressive representation is
\begin{align}
y(t) = \sum_{i=1}^n a_i y(t-i) + \sum_{i=0}^n b_i u(t-i)
\end{align}</p>
<p>where $a_i, b_i \in\mathbb{R}$. (An autonomous autoregressive model has no input terms.)</p>
<p>The current output $y(t)$ depends on the past $n$ outputs. Wherease the state-space representations contains only first-order differences, the autoregressive representation is expressed in higher-order differences.</p>
<p>Unlike in the convolutional model, there are finite terms here. The upper index $n$ is intentional notation &ndash; it turns out that when an $n$th order autoregressive model is converted to a state-space model we need an $n$-dimensional state vector.</p>
<h2 id="4-transfer-function-hq-of-shift-operator-q">4. Transfer function $H(q)$ of shift operator $q$</h2>
<p>We can rewrite the autoregressive equation in a slightly clever way. We turn calculus into algebra. The shift operator $q$ shifts a signal as in $qx(t) = x(t+1)$. And $q^i$ shifts $i$ times. The shift operator is the discrete-time analog of the continuous-time derivative operator. So, we can simply write</p>
<p>\begin{align}
y(t) - \sum_{i=1}^n a_i y(t-i) = \sum_{i=0}^n b_i u(t-i)
r(q)y(t) = s(q)u(t)
\end{align}
where $r(q) = (1 - a_1 q - \dots - a_n q^n)$ and $s(q) = (b_0 + b_1 q + \dots + b_n q^n)$ are polynomials in $q$.
And if we divide both sides by $r(q)$ we obtain
\begin{align}
y(t) = H(q)u(t)
\end{align}</p>
<p>where $H(q) = \frac{s(q)}{r(q)}$ is the <strong>transfer function</strong> in $q$.</p>
<p>For $m&gt;1, p&gt;1$, $H(q)$ is a $p\times m$ matrix, each entry of which is a rational function of $q$.</p>
<p>This is kinda of nice. We turned (discrete) <em>calculus</em> (difference equations) into <em>algebra</em>. There should be some lingering questions around whether we can just treat an operator $q$ as if it were an algebraic variable. For this, there is some extra rigor needed but I defer to ([1]).</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h4 id="from-ss---hq---autoregressive">From SS -&gt; $H(q)$ -&gt; Autoregressive</h4>
<p>Now for a fun part. Given an SS model we can turn it into $H(q)$, and if we have $H(q)$ we can just multiply the denominator polynomial on both sides, distribute the $q^i$ terms as shifts, resulting in the AR model.</p>
<p>We first specify the SS equations with $q$ and treat $q$ as an algebraic variable.
\begin{align}
qx(t) &amp;= Ax(t) + Bu(t) \\
x(t) &amp;= (qI - A)^{-1} Bu(t) \\
\end{align}
with output equation
\begin{align}
y(t) &amp;= (C(qI - A)^{-1} B + D)u(t) \\
y(t) &amp;= H(q) u(t)
\end{align}</p>
<p>The key observation is that matrix inverses such as $(qI - A)^{-1}$ can be written as $(qI-A)^{-1}=\frac{\text{adj}(qI-A)}{\text{det}(qI-A)}$, which imply both numerator and denominator reduce to polynomials of $q$ of degree at most $n$. And exact values of the polynomial coefficients can be derived (painstakingly) from the <a href="https://en.wikipedia.org/wiki/Adjugate_matrix">adjugate</a> and determinant of the matrix $qI-A$. We then can write out the full $H(q)=C(qI-A)^{-1} B + D$, where <em>each entry</em> of the matrix is a proper rational function of $q$. Each entry shares the same denominator polynomial, but different numerator polynomials.</p>
<p>It follows that $H(q)=C(qI-A)^{-1} B + D$ is a proper rational function of $q$. In the $p=m=1$ case, we write
\begin{align}
H(q) = \frac{b_n q^n + \dots + b_1 q + b_0}{a_n q^n + \dots + a_1 q + a_0}
\end{align}</p>
<p>This follows from the fact that an inverse matrix can be written as $(qI-A)^{-1}=\frac{\text{adj}(qI-A)}{\text{det}(qI-A)}$, which imply both numerator and denominator reduce to polynomials of $q$ of degree at most $n$. And exact values of the polynomial coefficients can be derived (painstakingly) from the <a href="https://en.wikipedia.org/wiki/Adjugate_matrix">adjugate</a> and determinant of the matrix $qI-A$.</p>
<p>Given that $H(q)$ is a proper rational function of $q$, we can multiply both sides of $y(t) = H(q)u(t)$ by the denominator polynomial and apply shift operators $q^i$ to obtain the autoregressive representation.</p>
<h4 id="from-ss---hq---convolution">From SS -&gt; H(q) -&gt; convolution</h4>
<p>We already derived the convolution representation above. But let&rsquo;s derive it from $H(q)$.</p>
<p>We can also note that the $(qI-A)^{-1}$ is the <a href="https://en.wikipedia.org/wiki/Resolvent_formalism">resolvent</a> of $A$ (but with $q$ instead of a complex variable). Reminding ourselves of <a href="https://en.wikipedia.org/wiki/Geometric_series">geometric series</a>, we can expand $(qI-A)^{-1}$ into an infinite series
\begin{align}
(qI-A)^{-1} &amp;= q(I - q^{-1}A)^{-1} \\
&amp;= \sum_{i=0}^\infty q^{-i} A^i
\end{align}
(Since $q$ is an operator this is really a <a href="https://en.wikipedia.org/wiki/Neumann_series">Neumann series</a>). Since each $q^{-i}$ applies $i$ negative shifts, we can rewrite the transfer function representaiton as
\begin{align}
y(t) = H(q)u(t)
y(t) = \sum_{i=1}^\infty CA^{i-1} B u(t-i) + Du(t)
\end{align}</p>
<!-- raw HTML omitted -->
<p>which is consistent with the convolution representation from section 2. above, but it&rsquo;s interesting to see the $h(t)$ parameters emerge from a different angle.</p>
<h3 id="5-transfer-function-hz-of-complex-variable-z">5. Transfer function $H(z)$ of complex variable $z$</h3>
<p>Most engineering textbooks omit the $H(q)$ representation and instead approach it from a slightly different viewpoint using the <a href="https://en.wikipedia.org/wiki/Z-transform">$z$-transform</a> instead of $q$. The steps are essentially identical due to how the $z$-transform acts on shifts.</p>
<p>If we apply the $z$-transform to the state-space equation we obtain
\begin{align}
X(z) = (zI - A)^{-1} BU(z)
\end{align}
where $X(z)$ and $U(z)$ are the $z$-transforms of $x(t)$ and $u(t)$.</p>
<p>We get
\begin{align}
Y(z) &amp;= (C(zI - A)^{-1} B + D)U(z) \\
&amp;= H(z) U(z)
\end{align}</p>
<p>Which is identical to the $H(q)$ but with complex variable $z$ in place of shift operator $q$.</p>
<h3 id="6-dtft-representation">6. DTFT representation</h3>
<p>For completeness we note that we can also take the DTFT
\begin{align}
Y(\omega) = H(\omega) U(\omega)
\end{align}
Where $Y(\omega)$ is the DTFT of $y(t)$ and is related to the $z$-transform via $z = e^{i\omega T}$. This allows us to define the <a href="https://en.wikipedia.org/wiki/Frequency_response">frequency response</a> of the system.</p>
<h2 id="realization-from-hq-to-ss">Realization, from $H(q)$ to SS</h2>
<p>Everything so far was done by starting with SS parameters.</p>
<p>If we start with either an autoregressive or convolution model, we can convert to an SS model. This is called <em>realization</em>.
To convert between any convolution or autoregressive representation to a state-space is called a <em>realization</em>. This is generally more involved but revolves around the construction of a <a href="https://en.wikipedia.org/wiki/Hankel_matrix">Hankel matrix</a>.</p>
<p>Assume we are given the convolution parameters $h(t)$ and construct the Hankel matrix:
\begin{align}
\mathcal{H} = \begin{bmatrix}
h(0) &amp; h(1) &amp; h(2) &amp; \dots &amp; h(n-1) \\
h(1) &amp; h(2) &amp; h(3) &amp; \dots &amp; h(n) \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
h(n-1) &amp; h(n) &amp; h(n+1) &amp; \dots &amp; h(2n-2)
\end{bmatrix}
\end{align}</p>
<p>We assume that $\mathcal{H}$ decomposes as</p>
<p>\begin{align}
\mathcal{H} = \begin{bmatrix} C \\ CA \\ \vdots \\ CA^{n-1} \end{bmatrix} \begin{bmatrix} B &amp; AB &amp; \dots &amp; A^{n-1}B \end{bmatrix}
\end{align}</p>
<p>The rank of $\mathcal{H}$ will be equal to $n$, the state dimension. So by taking the SVD of $\mathcal{H}$ we &ldquo;read out&rdquo; the $A,B,C$ matrices from the (top $n$) left singular vectors and (top $n$) right singular vectors accordingly. This defines system matrices up to a similarity transformation.</p>
<p>If instead we are given an autoregressive model we can take its corresponding proper rational function into an infinite series to obtain the convolution parameters $h(t)$ then construct the Hankel matrix.</p>
<p>We omitted many details, but this is the general idea. Realization is a large topic.</p>
<h2 id="fibonacci-example">Fibonacci example</h2>
<p>Consider the autonmous (no input) autoregressive model</p>
<p>\begin{align}
y(t) = y(t-1) + y(t-2)
\end{align}</p>
<p>with initial state (&ldquo;prompt&rdquo;) $y(0)=1, y(1)=1$.</p>
<p>Now consider the autonomous state-space model:</p>
<p>\begin{align}
x(t+1) = \begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 0 \end{bmatrix} x(t) \\
y(t) = \begin{bmatrix} 1 &amp; 0 \end{bmatrix} x(t)
\end{align}</p>
<p>with initial state $x(0) = [1, 1]^\top$.</p>
<p>Both models generate the Fibonacci sequence $1, 1, 2, 3, 5, 8, 13, \dots$.</p>
<h2 id="summary">Summary</h2>
<p>We intentionally omit some details and rigor. The point is to give a scaffolding of these connections. The texts [1], [2], and [3] provide the best exposition on this topic that I&rsquo;ve been able to find. The setup here is mostly inspired by [1]. Stanford&rsquo;s <a href="https://ee263.stanford.edu/archive/">EE263</a> is also an excellent course for learning about LTI systems.</p>
<p>Everything here is in discrete time. The continuous time case is analogous, with the Laplace transform taking place of the Z transform.</p>
<h2 id="takeaways">Takeaways</h2>
<ul>
<li>
<p>Although each architecture is capable of representing the same input-output mapping, they are distinct in terms of the number of parameters, parameter sensitivity, learning dynamics, etc. So from a learning standpoint one representation might be better than the other.</p>
</li>
<li>
<p>The deep learning literature gives first-class citizen status to architectures. The above derivations provide a different perspective: architectures are &ldquo;ways of writing down&rdquo; / parametrizing a mathematical model for sequences.</p>
</li>
<li>
<p>I wish I knew more about how these ideas hold up in nonlinear models. As far as I know, for any nonlinear autoregressive model there exists, somewhere, a corresponding nonlinear state-space realization.</p>
</li>
</ul>
<h2 id="references">References</h2>
<p>[1] Verhaegen, M., &amp; Verdult, V. (2007). Filtering and system identification: A least squares approach. Cambridge University Press.</p>
<p>[2] Antsaklis, P. J., &amp; Michel, A. N. (2007). A linear systems primer. Springer Science &amp; Business Media.</p>
<p>[3] Antoulas, A. C. (2005). Approximation of large-scale dynamical systems. Society for Industrial and Applied Mathematics.</p>
<p><a href="/">Back</a></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->

    </div>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false}
                ]
            });
        });
    </script>
</body>
</html>
