<!DOCTYPE html>
<html>
<head>
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-501N4WR52H"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-501N4WR52H');
    </script>

    <meta charset="UTF-8">
    <title>Jeffrey S. Seely</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="...">

    <link rel="stylesheet" href="/css/style.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŽˆ</text></svg>">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:wght@300;400&family=Source+Sans+Pro:wght@300;400&family=Source+Serif+4:wght@300;400&family=Source+Serif+Pro:wght@300;400&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI" crossorigin="anonymous">

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t" crossorigin="anonymous"></script>

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
</head>
<body>

    <div class="container">
        <h1>Equivalence of Linear State-Space and Autoregressive Models</h1>
        <p>Linear state-space models can be converted to linear autoregressive models and vice versa. This note derives these equivalence.</p>
<p>The line of work around the <a href="https://hazyresearch.stanford.edu/blog/2022-06-11-simplifying-s4">S4 paper</a>, the <a href="https://arxiv.org/abs/2302.10866">Hyena Heirarchy</a>,  any other related papers, has brought to attention (to the ML world) the equivalence of (linear) state-space and convolution models. There&rsquo;s also an equivalence to <em>autoregressive</em> models. The derivation is a bit more opaque; many texts rely on the Z-transform, but the derivation can understood via shift operators instead. We show this below.</p>
<p><img src="/static/images/ssar.png" alt="state-space and autoregressive model"></p>
<h2 id="state-space-model">State-Space Model</h2>
<p>A linear discrete-time state-space model is represented by:</p>
<p>\begin{align}
x(t+1) &amp;= Ax(t) + Bu(t) \\
y(t) &amp;= Cx(t) + Du(t)
\end{align}</p>
<p>where $u\in\mathbb{R}^m$ is the input, $x\in\mathbb{R}^n$ is the state, and $y\in\mathbb{R}^p$ is the output. The matrices $A, B, C,$ and $D$ are of appropriate dimensions.</p>
<h2 id="autoregressive-model">Autoregressive Model</h2>
<p>An $n$-th order autoregressive model is one where the current output $y(t)$ depends on the past $n$ outputs.</p>
<p>\begin{align}
y(t) = \sum_{i=1}^n \alpha_iy(t-i)
\end{align}</p>
<p>where $\alpha_i\in\mathbb{R}^{p\times p}$.</p>
<p>An autoregressive model with external inputs (also known as <strong>ARX</strong>) also depends on the current and past inputs $u(t)$.</p>
<p>\begin{align}
y(t) = \sum_{i=1}^n \alpha_iy(t-i) + \sum_{j=0}^n \beta_ju(t-j)
\end{align}</p>
<p>where $\beta_j\in\mathbb{R}^{p\times m}$. (Note the index ranges)</p>
<h2 id="conversion-from-state-space-to-autoregressive">Conversion: From State-Space to Autoregressive</h2>
<p>The goal is to convert state-space equations to ARX equations.</p>
<h3 id="step-1-rewrite-state-space-equations-in-terms-of-shift-operator">Step 1: Rewrite State-Space Equations in Terms of Shift Operator</h3>
<p>The shift operator $q$ advances a signal by one time step, so $qx(t) = x(t+1)$. Rewrite the state-space equations treating $q$ as a variable. We get:</p>
<p>\begin{align}
x(t+1) = qx(t) &amp;= Ax(t) + Bu(t)
\end{align}
i.e.,
\begin{align}
x(t) &amp;= (qI-A)^{-1}Bu(t)
\end{align}</p>
<p>Continuing this derivation, we obtain:</p>
<p>\begin{align}
y(t) &amp;= \left(C(qI-A)^{-1}B + D \right)u(t) \\
y(t) &amp;= H(q)u(t)
\end{align}</p>
<p>where $H(q)$ is the <strong>transfer function</strong>. These algebraic maniputations are valid if $(qI-A)^{-1}$ (an operator) is boundedly invertible and if $u(t)$ is bounded for all $t$.</p>
<h3 id="step-2-determine-hq">Step 2: Determine $H(q)$</h3>
<p>In $H(q)$ we have a inverse term $(qI-A)^{-1}$. We start by noting that a matrix inverse is a ratio of its <a href="https://en.wikipedia.org/wiki/Adjugate_matrix">adjugate</a> and determinant, thus</p>
<p>\begin{equation}
H(q) = \frac{C\operatorname{adj}(qI-A)B + D\operatorname{det}(qI-A)}{\operatorname{det}(qI-A)}
\end{equation}</p>
<p>By applying the <a href="https://en.wikipedia.org/wiki/Adjugate_matrix">adjugate</a> and <a href="https://en.wikipedia.org/wiki/Determinant">determinant</a> formulas, each entry of the matrix $H(q)$ can be shown to be a quotient of two 2nd order polynomials of $q$. In particular, they are proper rational functions of $q$ &mdash; the order of the numerator is less than or equal to the order of the denominator.</p>
<h3 id="step-3-re-arrange-and-apply-qi-operators-to-the-signal">Step 3: Re-arrange and Apply $q^i$ Operators to the Signal</h3>
<p>Each entry of $H(q)$ shares the same denominator but different numerators. For notational simplicity we assume $p=m=1$ (single-input, single-output). We can now re-arrange and apply higher-order shifts $q^i$s to obtain the autoregressive equation as:</p>
<p>\begin{align}
y(t+n) = \sum_{p=0}^n f_pu(t+p) - \sum_{p=1}^{n-1}g_py(t+p)
\end{align}</p>
<p>The coefficients $f_p,g_p\in\mathbb{R}$ can be expressed in terms of the state-space parameters $A,B,C,D$. The exact expressions can be obtained by applying the adjugate and determinant formulas.</p>
<h2 id="conversion-from-autoregressive-to-state-space">Conversion: From Autoregressive to State-Space</h2>
<p>Converting an autoregressive model to a state-space model is called the <strong>realization problem</strong>. Since we can apply similarity transforms to the state without affecting input-output behavior we know that there are infinitely many realizations of a given autoregressive model. One can choose a <a href="https://en.wikipedia.org/wiki/Realization_(systems)#Canonical_realizations">canonical realization</a>.</p>
<h2 id="relation-to-z-transform">Relation to Z-Transform</h2>
<p>The discrete-time analog of the Laplace transform is the <a href="https://en.wikipedia.org/wiki/Z-transform">Z-transform</a>, which converts a time-domain signal to a function in the complex number domain. The Z-transform of a shifted signal is represented as $\mathcal{Z}(x(t+1)) = zX(z)$. The entire derivation process above can be redone with $z$ in place of $q$, yielding $Y(z) = H(z)U(z)$, where $H(z)$ is also the transfer function.</p>
<h2 id="references">References</h2>
<p>[1] Verhaegen, M., &amp; Verdult, V. (2007). Filtering and system identification: A least squares approach. Cambridge University Press.</p>

    </div>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false}
                ]
            });
        });
    </script>
</body>
</html>
